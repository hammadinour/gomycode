{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm0vioNZTWCoq+RY5wWqM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hammadinour/gomycode/blob/main/checkpoint8final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#objectif 1 :\n",
        "import requests as re\n",
        "from bs4 import BeautifulSoup\n",
        "r= re.get (\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
        "\n",
        "soup = BeautifulSoup (r.content , 'html.parser')\n",
        "print(soup.title.string)\n"
      ],
      "metadata": {
        "id": "34VfaLdRZRaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#objectif 2 :\n",
        "import requests\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "enter_input = input ('search : ')\n",
        "u_i = string.capwords (enter_input)\n",
        "lists = u_i.split()\n",
        "word = \"_\".join(lists)\n",
        "\n",
        "\n",
        "url =\"https://en.wikipedia.org/wiki/\" + word\n",
        "\n",
        "\n",
        "def wikibot (url):\n",
        "  url_open = requests.get (url)\n",
        "  soup = BeautifulSoup(url_open.content , 'html.parser')\n",
        "  details = soup ('table',{'class':'infobox'})\n",
        "  for i in details :\n",
        "    h = i.find_all ('tr')\n",
        "    for j in h :\n",
        "      heading = j.find_all('th')\n",
        "      detail = j.find_all ('td')\n",
        "      dictt={}\n",
        "      if heading is not None and detail is not None :\n",
        "        for x,y in zip(heading,detail) :\n",
        "          dictt [x.text]= y.text\n",
        "          return (dictt)\n",
        "          #print (dictt)\n",
        "         # print(\"--------------------------------------------------------------\")\n",
        "    for i in range (1,3) :\n",
        "      print(soup('p')[i].text)\n",
        "\n",
        "wikibot(url)\n",
        "\n"
      ],
      "metadata": {
        "id": "UacNpaqsbiNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#objectif 3 :\n",
        "import requests as re\n",
        "from bs4 import BeautifulSoup\n",
        "r= re.get (input('insert your link'))\n",
        "\n",
        "\n",
        "soup = BeautifulSoup (r.content , 'html.parser')\n",
        "print('----usefull links :----')\n",
        "for link in soup.find_all('a') :\n",
        "  print(link.get('href'))"
      ],
      "metadata": {
        "id": "MgT3DU-Rgrz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#objectif 4 :\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = input('insert your link  :  ')\n",
        "\n",
        "\n",
        "def title (url) :\n",
        "  r= requests.get (url)\n",
        "  soup = BeautifulSoup (r.content , 'html.parser')\n",
        "  return(soup.title.string)\n",
        "\n",
        "def wikibot (url):\n",
        "   url_open = requests.get (url)\n",
        "   soup = BeautifulSoup(url_open.content , 'html.parser')\n",
        "   details = soup ('table',{'class':'infobox'})\n",
        "   for i in details :\n",
        "       h = i.find_all ('tr')\n",
        "       for j in h :\n",
        "         heading = j.find_all('th')\n",
        "         detail = j.find_all ('td')\n",
        "         if heading is not None and detail is not None :\n",
        "           for x,y in zip(heading,detail) :\n",
        "             print (\"{}  ::  {}\".format(x.text,y.text))\n",
        "             print(\"--------------------------------------------------------------\")\n",
        "   for i in range (1,3) :\n",
        "         print (soup('p')[i].text)\n",
        "#wikibot(url)\n",
        "\n",
        "def usefull_links (url) :\n",
        "  r= requests.get (url)\n",
        "  soup = BeautifulSoup (r.content , 'html.parser')\n",
        "  for link in soup.find_all('a') :\n",
        "    print (link.get('href'))\n",
        "\n",
        "\n",
        "def wrapper (url) :\n",
        "  return  title (url) , wikibot (url) , usefull_links (url)\n",
        "\n",
        "wrapper (url)\n",
        "#usefull_links (url)\n",
        "\n",
        "#wrapper ('https://en.wikipedia.org/wiki/Arabic')"
      ],
      "metadata": {
        "id": "iPXjlROA7oV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}